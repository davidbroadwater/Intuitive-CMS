__author__ = 'elisabethpaulson'
from twitter import *
import re
from nltk.corpus import stopwords
from collections import defaultdict
from gensim import corpora, models, similarities
from pprint import pprint


#oauth=OAuth('341804649-yJTL3nWemaTHSw3D4e80ALtgpLNsFTHVpuSuaxHJ','g5iGzcpY76FcBf5Gc9ilGPdZkDTIniOXAnuvnJ9s6tmlw','jpfq1LhAkc0b0WDJxiDSwV23H','HU8vFl8Zc6GolkN9IewfnQ2SzSAUwEbYKJrl2XbZGtv2giKZzP')

t=Twitter(auth=OAuth('341804649-yJTL3nWemaTHSw3D4e80ALtgpLNsFTHVpuSuaxHJ','g5iGzcpY76FcBf5Gc9ilGPdZkDTIniOXAnuvnJ9s6tmlw','jpfq1LhAkc0b0WDJxiDSwV23H','HU8vFl8Zc6GolkN9IewfnQ2SzSAUwEbYKJrl2XbZGtv2giKZzP'))

#t_stream=TwitterStream(auth=oauth)
#iterator=t_stream.statuses.sample()

#tweet_count=1000
#for tweet in iterator:
#    tweet_count-=1
#    print json.dumps(tweet)
#    if tweet_count<=0:
#        break

# FIND LAST 10,000 TWEETS RELATED TO FEMA
tweets=t.search.tweets(q="fema",result_type='recent',lang='en',count=10000)

# CREATE LIST OF TEXT FROM TWEETS
FEMAtweets=[]
for result in tweets["statuses"]:
    FEMAtweets.append(result["text"])

# STANDARDIZE TWEETS
std_FEMAtweets=[]
for x in FEMAtweets:
    y=x.upper()
    y=re.sub(r'[^A-Z#]',' ',y)
    y=re.sub(r'[\w]*RT[\w]*',r'',y)
    y=re.sub(r'[\w]*FEMA[\w]*',r'',y)
    y=re.sub(r'# ',r'',y)
    y=re.sub(r'@[\w]*','',y)
    y=re.sub(r' [A-Z] ','',y)
    if y!='':
        std_FEMAtweets.append(y)

# DELETE STOPWORDS
texts=[[word for word in sentences.lower().split() if word not in stopwords.words("english")] for sentences in std_FEMAtweets]

# CALCULATE FREQUENCY
frequency=defaultdict(int)
for text in texts:
    for token in text:
        frequency[token]+=1

# ONLY INCLUDE WORDS THAT APPEAR MORE THAN ONCE
texts=[[token for token in text if frequency[token]>1] for text in texts]

# SAVE DICTIONARY
dictionary=corpora.Dictionary(texts)
dictionary.save("FEMAtweets.dict")


#SAVE TWEETS AS CORPUS
corpus=[dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('FEMAtweets.mm',corpus)
corpus=corpora.MmCorpus("FEMAtweets.mm")

# TF-IDF
tfidf=models.TfidfModel(corpus)
corpus_tfidf=tfidf[corpus]

# LSI model for tweets related to FIMA
lsi=models.LsiModel(corpus_tfidf,id2word=dictionary,num_topics=10)
#corpus_lsi=lsi[corpus_tfidf]
pprint(lsi.print_topics(10)) #print first 10 topics

# TRY USING HDP MODEL INSTEAD OF LSI
#hdp=models.HdpModel(corpus,id2word=dictionary)
#pprint(hdp.print_topics(10))


# IMPORT CORPUS FROM FEMA WEBSITE
corpus=corpora.MmCorpus("FEMA.mm")

# TF-IDF
tfidf=models.TfidfModel(corpus)
corpus_tfidf=tfidf[corpus]

#Run LSI or HDP model on corpus from FEMA website. Uses topics generated by tweets and scores FEMA content according to these topics
#corpus_hdp=hdp[corpus_tfidf]
corpus_lsi=lsi[corpus_tfidf]


m=-1
for doc in corpus_lsi: #scans through content on FEMA's site
    m=m+1
    doc=sorted(doc,key=lambda  item: -item[1])
    if doc!=[]:
        if doc[0][1]>.6: #specify a similarity score threshold
            print'Document ',m,' relates to topic ', doc[0][0]