__author__ = 'elisabethpaulson'
from twitter import *
import re
from collections import defaultdict
from nltk import corpus
from gensim import corpora, models, similarities
from pprint import pprint
from FEMA_text_analysis_gensim import *
from elasticsearch import Elasticsearch
es=Elasticsearch()
import nltk

# MUST HAVE RUN GatherTweets.py BEFORE THIS TO CREATE THE ELASTICSEARCH DATABASE

tweets=[]
for m in range(1000):
    res = es.get(index="sentiment", doc_type='hurricanetweets',id=m)
    tweets.append(res['_source']['message'])
for m in range(1000):
    res = es.get(index="sentiment", doc_type='FEMAtweets',id=m)
    tweets.append(res['_source']['message'])
for m in range(1000):
    res = es.get(index="sentiment", doc_type='floodtweets',id=m)
    tweets.append(res['_source']['message'])
for m in range(1000):
    res = es.get(index="sentiment", doc_type='emergencytweets',id=m)
    tweets.append(res['_source']['message'])
pprint(tweets[:10])
pprint(tweets[1000:1010])
std_FEMAtweets=[]
for x in tweets:
    y=x.upper()
    y=re.sub(r'[^A-Z#]',' ',y)
    y=re.sub(r'[\w]*RT[\w]*',r' ',y)
    y=re.sub(r'[\w]*FEMA[\w]*',r' ',y)
    y=re.sub(r'# ',r' ',y)
    y=re.sub(r'@[\w]*','',y)
    y=re.sub(r' [A-Z] ','',y)
    y.replace('HTTP','')
    y.replace('  ',' ')
    if y!='':
        std_FEMAtweets.append(y)


texts=[nltk.word_tokenize(tweet) for tweet in std_FEMAtweets]
texts=[[word for word in sentences if word.lower() not in stopwords.words("english")] for sentences in texts]

# KEEP ONLY NOUNS
texts=[nltk.pos_tag(sentences) for sentences in texts]
texts=[[word for word in sentences if word[1]=='NNP']for sentences in texts]
texts=[[word[0] for word in sentences] for sentences in texts]
#print(texts[:5])
#texts=[[word for word in sentences if d.check(word)] for sentences in texts]
#print(texts[:5])

# REDUCE NOUNS TO STEMS
# from nltk.stem.snowball import SnowballStemmer
# st=SnowballStemmer('english')
# texts=[[st.stem(word) for word in sentences] for sentences in texts]

# SAVE DICTIONARY
dictionary=corpora.Dictionary(texts)
dictionary.save("FEMAtweets.dict")

#SAVE TWEETS AS CORPUS
corpus=[dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('FEMAtweets.mm',corpus)
corpus=corpora.MmCorpus("FEMAtweets.mm")

# TF-IDF
tfidf=models.TfidfModel(corpus)
corpus_tfidf=tfidf[corpus]

# LSI model for tweets related to FIMA
#lsi=models.LsiModel(corpus_tfidf,id2word=dictionary,num_topics=15)
#corpus_lsi=lsi[corpus_tfidf]
#pprint(lsi.print_topics(5)) #print first 10 topics

# LSI model for tweets related to FIMA
lda=models.LdaModel(corpus_tfidf,id2word=dictionary,num_topics=15)
#corpus_lsi=lsi[corpus_tfidf]
pprint(lda.print_topics(5)) #print first 10 topics

# TRY USING HDP MODEL INSTEAD OF LSI
#hdp=models.HdpModel(corpus,id2word=dictionary)
#pprint(hdp.print_topics(10))


# IMPORT CORPUS FROM FEMA WEBSITE
corpus=corpora.MmCorpus("FEMA.mm")

# TF-IDF
tfidf=models.TfidfModel(corpus)
corpus_tfidf=tfidf[corpus]

#Run LSI, HDP, or LDA model on corpus from FEMA website. Uses topics generated by tweets and scores FEMA content according to these topics
#corpus_hdp=hdp[corpus_tfidf]
corpus_lda=lda[corpus_tfidf]


m=-1
for doc in corpus_lsi: #scans through content on FEMA's site
    m=m+1
    doc=sorted(doc,key=lambda  item: -item[1])
    if doc!=[]:
        if doc[0][1]>.4: #specify a similarity score threshold
            print standard_links[m],' relates to topic ', doc[0][0]